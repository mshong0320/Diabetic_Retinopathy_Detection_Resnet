{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import OrderedDicted to corectly align the network layers\n",
    "#import nn you use activation and dropout features\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data\n",
    "#from sampler import ImbalancedDatasetSampler\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from IPython import display\n",
    "from skimage import io, transform\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.image as mpimg\n",
    "import scipy\n",
    "from scipy import ndimage, misc\n",
    "from scipy.misc import imshow\n",
    "import skimage\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import shutil\n",
    "import pdb\n",
    "import cv2\n",
    "\n",
    "plt.ion()\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(2, 2, 3)\n",
    "# print(x)\n",
    "# print(x.size())\n",
    "# x=x.view(3, 2,-1)\n",
    "# print(x)\n",
    "# print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels = pd.read_csv(\"/home/jupyter/kaggle_data/trainLabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "            \n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        dataset_type = type(dataset)\n",
    "        if dataset_type is torchvision.datasets.MNIST:\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
    "            return dataset.imgs[idx][1]\n",
    "        else:\n",
    "            return dataset.imgs[idx][1]\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is copied from the works of pytorch (source: https://chsasank.github.io/vision/_modules/torchvision/datasets/folder.html)\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for target in sorted(os.listdir(dir)): # all the directories of classes\n",
    "        d = os.path.join(dir, target) # class directories are the targets\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)): \n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class DiabeticRetinoFolder(data.Dataset):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 loader=default_loader):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root, class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        \n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            #img = img.numpy()\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            #target = target.numpy()\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/jupyter/kaggle_data/Train'\n",
    "batch_size = 16\n",
    "\n",
    "def dataset_DR(datadir, valid_size, batchsize):\n",
    "    TRAIN_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    TRAIN_STD = np.array([0.229, 0.224, 0.225]) \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(size=1024),\n",
    "        transforms.Resize(1024),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(TRAIN_MEAN, TRAIN_STD)])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(size=1024),\n",
    "        #transforms.CenterCrop(size=256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(TRAIN_MEAN, TRAIN_STD)])\n",
    "    \n",
    "    train_data = DiabeticRetinoFolder(datadir, transform=train_transform)\n",
    "    test_data = DiabeticRetinoFolder(datadir, transform=test_transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_data, sampler=ImbalancedDatasetSampler(train_data), batch_size=batchsize, num_workers = 4)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, sampler=ImbalancedDatasetSampler(test_data), batch_size=batchsize, num_workers = 4)\n",
    "    \n",
    "    train_size = int((1-valid_size) * len(train_data))\n",
    "    test_size = len(train_data) - train_size\n",
    "    print(\"train and test sizes: \", train_size, \",\", test_size)\n",
    "    return train_data, test_data, trainloader, testloader, train_size, test_size\n",
    "\n",
    "Trainset, Testset, TrainLoad, TestLoad, train_size, test_size = dataset_DR(data_dir, 0.1, batch_size)\n",
    "classes = ('Mild_NPDR', 'Moderate_NPDR', 'Normal', 'PDR', 'Severe_NPDR')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(TestLoad)\n",
    "%time image_batch, label_batch = next(test_iter) \n",
    "print(image_batch.shape, image_batch.type()) \n",
    "print(label_batch.shape, label_batch.type()) \n",
    "TRAIN_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "TRAIN_STD = np.array([0.229, 0.224, 0.225]) \n",
    "grid = torchvision.utils.make_grid(image_batch, nrow=8).cpu().numpy().transpose(1, 2, 0)\n",
    "grid = TRAIN_STD * grid + TRAIN_MEAN\n",
    "# The min and max values are very close to 0.0 and 1.0, but\n",
    "# are just outside because of numerical effects.\n",
    "grid = np.clip(grid, 0.0, 1.0)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(grid)\n",
    "#print(label_batch)\n",
    "\n",
    "label_batch = label_batch.numpy().flatten()-1\n",
    "print(label_batch)\n",
    "print(' '.join('%4s' % classes[label_batch[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = models.resnet50(pretrained=True)\n",
    "v.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrunkRes(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, orig_resnet):\n",
    "    super().__init__()\n",
    "    self.orig_resnet = orig_resnet\n",
    "    child_counter = 0\n",
    "    for child in orig_resnet.children():\n",
    "        if child_counter < 9:\n",
    "            #print(\"child \", child_counter, \" was frozen\")\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif child_counter == 9:\n",
    "            continue\n",
    "        child_counter +=1\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3,64,7,stride=2,padding=3,bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.maxpool = nn.MaxPool2d(3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "    self.new_model = nn.Sequential(*list(orig_resnet.children())[:-1])\n",
    "  \n",
    "  def forward(self, x):\n",
    "    #print(\"here: \",x.size())\n",
    "    x = self.new_model(x)\n",
    "    #print(\"there\", x.size())\n",
    "#     x = self.conv1(x)\n",
    "#     x = self.bn1(x)\n",
    "#     x = self.relu(x)\n",
    "#     x = self.maxpool(x)\n",
    "    \n",
    "#     x = self.orig_resnet.layer1(x) #torch.Size([1, 256, 64, 64])\n",
    "# #    print(\"layer1 output: \", x.size())\n",
    "#     x = self.orig_resnet.layer2(x) #torch.Size([1, 512, 32, 32])\n",
    "# #    print(\"layer2 output: \", x.size())\n",
    "#     x = self.orig_resnet.layer3(x) # torch.Size([1, 1024, 16, 16])\n",
    "# #    print(\"layer3 output: \", x.size())\n",
    "#     x = self.orig_resnet.layer4(x) # torch.Size([1, 2048, 8, 8])\n",
    "# #    print(\"layer4 output: \", x.size())\n",
    "#     x = self.orig_resnet.avgpool(x) # torch.Size([1, 2048, 2, 2])\n",
    "    #print(\"final output: \", x.size())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up my model with final layer customized\n",
    "class HugeResnet(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        #num_final_in = pretrained_model.fc.in_features\n",
    "        \n",
    "        self.final_linear = torch.nn.Linear(131072, 5) # TODO\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"new shape: \", x.size())\n",
    "        x = self.final_linear(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        #print(\"final shape: \", x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting up my model with final layer customized\n",
    "# class HugeResnet(torch.nn.Module):\n",
    "#     def __init__(self, orig_resnet):\n",
    "#         super().__init__()\n",
    "#         self.orig_resnet = orig_resnet\n",
    "#         #num_final_in = pretrained_model.fc.in_features\n",
    "#         self.final_linear = torch.nn.Linear(1384448, 5) # TODO\n",
    "#         self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "#     def forward(self, x):\n",
    "#         x = self.orig_resnet.conv1(x)\n",
    "#         x = self.orig_resnet.bn1(x)\n",
    "#         x = self.orig_resnet.relu(x)\n",
    "#         x = self.orig_resnet.maxpool(x)\n",
    "\n",
    "#         x = self.orig_resnet.layer1(x)\n",
    "#         x = self.orig_resnet.layer2(x)\n",
    "#         x = self.orig_resnet.layer3(x)\n",
    "#         x = self.orig_resnet.layer4(x)\n",
    "\n",
    "#         x = self.orig_resnet.avgpool(x)    \n",
    "#         #print(x.size())\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         #print(\"new shape: \", x.size())\n",
    "#         x = self.final_linear(x)\n",
    "#         x = self.logsoftmax(x)\n",
    "#         #print(\"final shape: \", x.size())\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_on_sections(model, img):\n",
    "    logits = []\n",
    "    img = np.moveaxis(img, 0, -1) ##(1024, 1024, 3)\n",
    "    for j in range(4):\n",
    "        j = j+1\n",
    "        for k in range(4):\n",
    "            k = k+1\n",
    "            img_sec = img[256*(k-1):256*k,256*(j-1):256*j,:]\n",
    "            np_img = np.moveaxis(img_sec,2,0) #(3, 256, 256)\n",
    "            np_img = torch.FloatTensor(np_img) # convert np array to tensor\n",
    "            np_img= np_img.unsqueeze(0) # converts to a single batch torch.Size([1, 3, 256, 256])\n",
    "            with torch.no_grad():\n",
    "                Logit = model(np_img)\n",
    "                Logit = np.array(Logit)\n",
    "                Logit = np.reshape(Logit,(1,2,2,2048)) # (1, 2, 2, 2048)\n",
    "                logits.append(Logit) \n",
    "    # once all 16 logits are collected per item in a logit list, so concat into a batchsizex31024x1024\n",
    "    logits = np.array(logits) # (16, 1, 2, 2, 2048)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(3, 512, 512)\n",
    "# print(x.size())\n",
    "\n",
    "# x = x[0:3, 0:256, 0:256]\n",
    "# print(x.size())\n",
    "\n",
    "# x = x[0:3, 0:256, 0:512]\n",
    "# print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_one_epoch(resn, big_resn):\n",
    "    big_resn.eval()\n",
    "    resn.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    count = 0\n",
    "    Loss = []\n",
    "    ER = [] #error rates\n",
    "    for i, data in enumerate(TrainLoad, 0):\n",
    "        count += 1\n",
    "        image_batch, label_batch = data\n",
    "        image_batch = Variable(image_batch.cuda())\n",
    "        label_batch = Variable(label_batch.cuda())\n",
    "        np_imgs = image_batch.cpu().numpy()\n",
    "        label_batch = label_batch.cpu().numpy()\n",
    "        label_batch = label_batch-1\n",
    "        label_batch = Variable(torch.FloatTensor(label_batch).cuda())\n",
    "        p_batch = label_batch.long()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        Logitcollect = [] # batchsize units of logit terms\n",
    "        for i in np_imgs: # per image in a batch\n",
    "            Logits = resnet_on_sections(resn,i) #(16,1,2,2,2048)\n",
    "            # once all 16 logits are collected per item in a logit list, so concat into a batchsizex31024x1024\n",
    "            Logitcollect.append(Logits)\n",
    "\n",
    "        input_batch = np.array(Logitcollect)\n",
    "        # will have collected batch_size # of Logits in the Logitcollect list\n",
    "        \n",
    "        input_batch = Variable(torch.FloatTensor(input_batch).cuda())\n",
    "#        print(input_batch.size()) # torch.Size([batchsize,16,1,2,2,2048])\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = big_resn(input_batch).cuda()\n",
    "            _, preds = torch.max(logits,1)\n",
    "            #print(logits.size())\n",
    "            #print(p_batch.size())\n",
    "            #print(p_batch)\n",
    "            \n",
    "            loss = criterion(logits, p_batch).cuda()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print statistics\n",
    "            #print(preds)\n",
    "            #print(preds.size())\n",
    "            #print(label_batch.size())\n",
    "            error_rate = 1.0 - (preds == label_batch.long()).float().mean()\n",
    "            running_loss += loss.item() \n",
    "            running_corrects += torch.sum(preds == label_batch.long())\n",
    "            #ER.append(error_rate)\n",
    "            #print(i%1000 == 999)\n",
    "#             if np.any(i % 1000 == 999):   # print every 1000 mini-batches\n",
    "#                 print('[%5d] loss: %.3f' %(i+1, running_loss / 1000))\n",
    "#                 running_loss = 0.0\n",
    "    \n",
    "#        mean_loss = float(sum(Loss))/float(count)\n",
    "#        mean_error_rate=float(sum(ER))/float(count)\n",
    "        print(\"training_error_rate: \", error_rate)\n",
    "        return loss.item(), error_rate.item(), running_loss, running_corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(resn, big_resn):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    big_resn.eval()\n",
    "    resn.eval()\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    count = 0\n",
    "    ER = [] #error rates\n",
    "    Loss = []\n",
    "    for input_batch, label_batch in TestLoad:\n",
    "        count += 1\n",
    "        #input_batch = Variable(input_batch.cuda())\n",
    "        np_imgs = input_batch.numpy()\n",
    "        label_batch = Variable(label_batch.cuda())\n",
    "        label_batch = label_batch.cpu().numpy()\n",
    "        label_batch = label_batch-1\n",
    "        label_batch = Variable(torch.FloatTensor(label_batch).cuda())\n",
    "        p_batch = label_batch.long()\n",
    "        \n",
    "        Logitcollect = [] # batchsize units of logit terms\n",
    "\n",
    "        for i in np_imgs: # for images in a batch\n",
    "            Logits = resnet_on_sections(resn,i) #(16,1,2,2,2048)\n",
    "            # once all 16 logits are collected per item in a logit list, so concat into a batchsizex31024x1024\n",
    "            Logitcollect.append(Logits)\n",
    "            \n",
    "        outputs = np.array(Logitcollect)\n",
    "        outputs = Variable(torch.FloatTensor(outputs).cuda())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = big_resn(outputs).cuda()\n",
    "            _, preds = torch.max(logits,1)\n",
    "            loss = criterion(logits, p_batch).cuda()\n",
    "            \n",
    "            #print statistics\n",
    "            error_rate = 1.0 - (preds == label_batch.long()).float().mean()\n",
    "            val_running_loss += loss.item()\n",
    "            val_running_corrects += torch.sum(preds == label_batch.long())\n",
    "            total += label_batch.size(0)\n",
    "            correct += (preds == label_batch.long()).sum().item()\n",
    "            \n",
    "        print(\"Val_error_rate: \", error_rate)\n",
    "        print('Accuracy of the network on the 841 test images: %d %%' % (100 * correct / total))\n",
    "        return loss.item(), error_rate, val_running_loss, val_running_corrects,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50 = models.resnet50(pretrained=True)\n",
    "MM = TrunkRes(res50)\n",
    "\n",
    "tm = HugeResnet(v)\n",
    "tm.to(device);\n",
    "\n",
    "optimizer = optim.SGD(tm.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "weights = torch.tensor([0.24, 0.134, 0.03, 0.811, 1])\n",
    "class_weights=torch.FloatTensor(weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "epoch = 0\n",
    "epochs = []\n",
    "train_losses, train_errors, train_run_losses, train_run_corrects = [], [], [], []\n",
    "val_losses, val_errors, val_run_losses, val_run_corrects = [], [], [], []\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(tm.state_dict())\n",
    "tm.load_state_dict(best_model_wts)\n",
    "Best_model_Wts = copy.deepcopy(MM.state_dict())\n",
    "MM.load_state_dict(Best_model_Wts)\n",
    "\n",
    "Time = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    since = time.time()\n",
    "    \n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 40)\n",
    "    \n",
    "    train_loss_epoch, mean_train_error_epoch, Train_runn_loss, Train_runn_corrects = train_one_epoch(MM, tm)\n",
    "    val_loss_epoch, mean_val_error_epoch, Val_runn_loss, Val_runn_corrects = val(MM, tm)\n",
    "    \n",
    "    epoch += 1\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    train_losses.append(train_loss_epoch)\n",
    "    train_errors.append(mean_train_error_epoch)\n",
    "    train_run_losses.append(Train_runn_loss)\n",
    "    train_run_corrects.append(Train_runn_corrects)\n",
    "#    train_epoch_loss = Train_runn_loss\n",
    "    print(' Train # of Corrects : {:.4f} for a batchsize of : {:.4f}'.format(Train_runn_corrects, batch_size))\n",
    "    train_epoch_acc = float(Train_runn_corrects) / float(batch_size)\n",
    "    print(' Train Loss: {:.4f} Train Acc: {:.4f}'.format(train_loss_epoch, train_epoch_acc))\n",
    "    \n",
    "    val_losses.append(val_loss_epoch)\n",
    "    val_errors.append(mean_val_error_epoch)\n",
    "    val_run_losses.append(Val_runn_loss)\n",
    "    val_run_corrects.append(Val_runn_corrects)\n",
    "#    val_epoch_loss = Val_runn_loss\n",
    "#    print(' Validation # of Corrects per batch : {:.4f}'.format(Val_runn_corrects))\n",
    "    val_epoch_acc = float(Val_runn_corrects) / float(batch_size)\n",
    "    print(' Val Loss: {:.4f} Val Acc: {:.4f}'.format(val_loss_epoch, val_epoch_acc))\n",
    "    print(f'Epoch {epoch:3d}. Approx. train error rate: {mean_train_error_epoch:.3f}. Val error rate: {mean_val_error_epoch:.3f}.')\n",
    "    \n",
    "    if val_epoch_acc > best_acc:\n",
    "        best_acc = val_epoch_acc\n",
    "        best_model_wts = copy.deepcopy(tm.state_dict())\n",
    "        tm.load_state_dict(best_model_wts)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Epoch complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    Time.append(time_elapsed)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "ax[0].plot(train_losses, label='train loss')\n",
    "ax[0].plot(val_losses, label='val loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(train_errors, label='train error rate')\n",
    "ax[1].plot(val_errors, label='val error rate')\n",
    "ax[1].legend()\n",
    "plt.grid(True)\n",
    "\n",
    "Totaltime = sum(Time)\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(Totaltime // 60, Totaltime % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_run_corrects, label='# Correct Training prediction')\n",
    "plt.plot(val_run_corrects, label='Validation prediction correct #')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"# of corrects\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(TestLoad)\n",
    "images, labels = dataiter.next()\n",
    "Labels = Variable(labels.cuda())\n",
    "label_batch = Labels.float()\n",
    "grid = torchvision.utils.make_grid(images, nrow=8).cpu().numpy().transpose(1, 2, 0)\n",
    "grid = TRAIN_STD * grid + TRAIN_MEAN\n",
    "\n",
    "# The min and max values are very close to 0.0 and 1.0, but\n",
    "# are just outside because of numerical effects.\n",
    "grid = np.clip(grid, 0.0, 1.0)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(grid)\n",
    "label_batch = label_batch.cpu().numpy()\n",
    "label_batch = label_batch-1\n",
    "label_batch = Variable(torch.FloatTensor(label_batch).cuda())\n",
    "label_batch = label_batch.int()\n",
    "print(label_batch)\n",
    "print(' '.join('%5s' % classes[label_batch[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(5))\n",
    "class_total = list(0. for i in range(5))\n",
    "\n",
    "for input_batch, label_batch in TestLoad:\n",
    "    np_imgs = input_batch.numpy()\n",
    "    label_batch = Variable(label_batch.cuda())\n",
    "    label_batch = label_batch.cpu().numpy()\n",
    "    label_batch = label_batch-1\n",
    "    label_batch = Variable(torch.FloatTensor(label_batch).cuda())\n",
    "    p_batch = label_batch.long()\n",
    "\n",
    "    Logitcollect = [] # batchsize units of logit terms\n",
    "\n",
    "    for i in np_imgs: # for images in a batch\n",
    "        Logits = resnet_on_sections(MM,i) #(16,1,2,2,2048)\n",
    "        Logitcollect.append(Logits)\n",
    "\n",
    "    outputs = np.array(Logitcollect)\n",
    "    outputs = Variable(torch.FloatTensor(outputs).cuda())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = tm(outputs).cuda()\n",
    "        _, preds = torch.max(logits,1)   \n",
    "        c = (preds == label_batch.long()).squeeze()\n",
    "        if len(label_batch) > 0:\n",
    "            for i in range(len(c)):\n",
    "                label = label_batch[i]\n",
    "                print(c)\n",
    "                class_correct[label.int().item()] += c[i].int().item()\n",
    "                class_total[label.int().item()] += 1\n",
    "        elif len(c) == 0:\n",
    "            label = label_batch[0]\n",
    "            class_correct[label.int().item()] += c.item()\n",
    "            class_total[label.int().item()] += 1\n",
    "\n",
    "        print(class_correct)\n",
    "        print(class_total)\n",
    "\n",
    "        for i in range(5):\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted: ', ' '.join('%5s' % classes[preds[j]] for j in range(16)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
